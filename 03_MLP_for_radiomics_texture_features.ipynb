{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole feature : (270, 854)\n",
      "Whole labels : (270,)\n",
      "Whole subjects : 270\n",
      "Number by labels : Counter({1: 92, 2: 90, 0: 88})\n"
     ]
    }
   ],
   "source": [
    "# Extract Texture features\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "texture_features = ['../result/left_mask.csv', '../result/right_mask.csv']\n",
    "label_file = '../data/label_dict.pickle'\n",
    "clinical_file = '../data/clinical_data.pickle'\n",
    "\n",
    "with open(label_file, 'rb') as file: label_dict = pickle.load(file)\n",
    "with open(clinical_file, 'rb') as file: clinical_dict = pickle.load(file)\n",
    "    \n",
    "feature_names = np.array(list(pd.read_csv(texture_features[0]).columns)[1:])\n",
    "\n",
    "whole_feature = []\n",
    "whole_label = []\n",
    "whole_subjects = []\n",
    "\n",
    "for texture_feature in texture_features:\n",
    "#     print(texture_feature)\n",
    "    \n",
    "    for key, value in pd.read_csv(texture_feature).iterrows():\n",
    "\n",
    "        subject = '_'.join(list(value)[0].split('_')[:-1])\n",
    "        label = label_dict[subject]\n",
    "        clinic_data = clinical_dict[subject] # is male, is female, age\n",
    "\n",
    "#         print(key, list(value)[0], len(list(value[1:])), label, clinic_data)\n",
    "        \n",
    "        whole_feature.append(list(value[1:]) + clinic_data)\n",
    "        whole_label.append(label)\n",
    "        whole_subjects.append(list(value)[0])\n",
    "\n",
    "#         break\n",
    "\n",
    "whole_feature = np.array(whole_feature)\n",
    "whole_label = np.array(whole_label)\n",
    "        \n",
    "print('Whole feature :',np.array(whole_feature).shape) # (270 = 135 + 135, 854 = 851 + 3)\n",
    "print('Whole labels :', np.array(whole_label).shape)\n",
    "print('Whole subjects :', len(whole_subjects))\n",
    "print('Number by labels :',Counter(whole_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ad_dataset(object):\n",
    "    def __init__(self, binary = False, phase='train'):\n",
    "        \n",
    "        self.phase = phase\n",
    "        \n",
    "        self.whole_feature, self.whole_label, self.subjects = self.read_data(binary=binary)\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            self.label_file = label_file\n",
    "            self.label_dict = self.read_pickle(self.label_file)\n",
    "            self.image_ids = list(self.label_dict.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.whole_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return {'data' : self.ToTensor(self.whole_feature[index]),\n",
    "                'label' : int(self.whole_label[index]),\n",
    "                'name' : self.subjects[index]}\n",
    "    \n",
    "    def ToTensor(self, data):\n",
    "        return torch.from_numpy(data).float()\n",
    "    \n",
    "    def read_pickle(self, file):\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_file = pickle.load(f)\n",
    "            \n",
    "        return loaded_file\n",
    "    \n",
    "    def read_data(self, binary = False):\n",
    "        texture_features = ['../result/left_mask.csv', '../result/right_mask.csv']\n",
    "        label_file = '../data/label_dict.pickle'\n",
    "        clinical_file = '../data/clinical_data.pickle'\n",
    "        \n",
    "        label_dict = self.read_pickle(label_file)\n",
    "        clinical_dict = self.read_pickle(clinical_file)\n",
    "\n",
    "        feature_names = np.array(list(pd.read_csv(texture_features[0]).columns)[1:])\n",
    "\n",
    "        whole_feature = []\n",
    "        whole_label = []\n",
    "        whole_subjects = []\n",
    "\n",
    "        for texture_feature in texture_features:\n",
    "        #     print(texture_feature)\n",
    "\n",
    "            for key, value in pd.read_csv(texture_feature).iterrows():\n",
    "\n",
    "                subject = '_'.join(list(value)[0].split('_')[:-1])\n",
    "                label = label_dict[subject]\n",
    "                clinic_data = clinical_dict[subject] # is male, is female, age\n",
    "                \n",
    "                if binary:\n",
    "                    if label == 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if label == 2:\n",
    "                            label = 1 \n",
    "                        whole_feature.append(list(value[1:]) + clinic_data)\n",
    "                        whole_label.append(label)\n",
    "                        whole_subjects.append(list(value)[0])\n",
    "                        \n",
    "                else:\n",
    "                    whole_feature.append(list(value[1:]) + clinic_data)\n",
    "                    whole_label.append(label)\n",
    "                    whole_subjects.append(list(value)[0])\n",
    "\n",
    "        whole_feature = np.array(whole_feature)\n",
    "        whole_label = np.array(whole_label)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(whole_feature)\n",
    "        whole_feature = scaler.transform(whole_feature)\n",
    "\n",
    "        print('Whole feature :',np.array(whole_feature).shape) # (270 = 135 + 135, 854 = 851 + 3)\n",
    "        print('Whole labels :', np.array(whole_label).shape)\n",
    "        print('Whole subjects :', len(whole_subjects))\n",
    "        print('Number by labels :',Counter(whole_label))\n",
    "\n",
    "        return whole_feature, whole_label, whole_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole feature : (178, 854)\n",
      "Whole labels : (178,)\n",
      "Whole subjects : 178\n",
      "Number by labels : Counter({1: 90, 0: 88})\n",
      "torch.Size([1, 854]) tensor([1]) ['082_S_1079_R']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ad_dataset(binary=True)\n",
    "# print(dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    \n",
    "    x = batch['data']\n",
    "    y = batch['label']\n",
    "    name = batch['name']\n",
    "    \n",
    "    print(x.size(), y, name)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def binarize(label):\n",
    "    x = np.zeros(2, dtype=np.float)\n",
    "    x[label] = 1\n",
    "    return torch.Tensor(x)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=1):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch in dataloaders[phase]:\n",
    "                \n",
    "                inputs = batch['data']\n",
    "                labels = batch['label']\n",
    "\n",
    "                inputs = torch.squeeze(inputs) # 1 * 854 => 854\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 0)\n",
    "\n",
    "                    \n",
    "                    outputs = torch.unsqueeze(outputs, 0)\n",
    "                    \n",
    "                    labels = binarize(labels)\n",
    "                    labels = torch.unsqueeze(labels, 0)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                if preds.item() == int(batch['label']):\n",
    "                    running_corrects += 1\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Softmax, ReLU, Sigmoid\n",
    "from torch.nn.modules.loss import BCELoss, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "\n",
    "ad_model = torch.nn.Sequential(\n",
    "    \n",
    "    Linear(in_features=854, out_features=256, bias=False),\n",
    "    ReLU(),\n",
    "    \n",
    "    Linear(in_features=256, out_features=128, bias=False),\n",
    "    ReLU(),\n",
    "    \n",
    "    Linear(in_features=128, out_features=2, bias=False),\n",
    "    Sigmoid()\n",
    "#     Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole feature : (178, 854)\n",
      "Whole labels : (178,)\n",
      "Whole subjects : 178\n",
      "Number by labels : Counter({1: 90, 0: 88})\n",
      "Epoch 1/150\n",
      "----------\n",
      "train Loss: 488.6038 Acc: 0.7042\n",
      "val Loss: 417.6018 Acc: 0.8333\n",
      "\n",
      "Epoch 2/150\n",
      "----------\n",
      "train Loss: 435.2228 Acc: 0.7606\n",
      "val Loss: 515.6987 Acc: 0.8056\n",
      "\n",
      "Epoch 3/150\n",
      "----------\n",
      "train Loss: 291.5513 Acc: 0.8380\n",
      "val Loss: 261.6161 Acc: 0.7778\n",
      "\n",
      "Epoch 4/150\n",
      "----------\n",
      "train Loss: 661.2816 Acc: 0.8310\n",
      "val Loss: 496.6759 Acc: 0.7222\n",
      "\n",
      "Epoch 5/150\n",
      "----------\n",
      "train Loss: 588.9488 Acc: 0.8099\n",
      "val Loss: 1105.7948 Acc: 0.7500\n",
      "\n",
      "Epoch 6/150\n",
      "----------\n",
      "train Loss: 1055.7388 Acc: 0.7606\n",
      "val Loss: 1123.8763 Acc: 0.5000\n",
      "\n",
      "Epoch 7/150\n",
      "----------\n",
      "train Loss: 2179.3049 Acc: 0.6268\n",
      "val Loss: 1747.7667 Acc: 0.3333\n",
      "\n",
      "Epoch 8/150\n",
      "----------\n",
      "train Loss: 2524.4809 Acc: 0.5634\n",
      "val Loss: 1115.8606 Acc: 0.2778\n",
      "\n",
      "Epoch 9/150\n",
      "----------\n",
      "train Loss: 1482.9801 Acc: 0.5211\n",
      "val Loss: 1148.8647 Acc: 0.3333\n",
      "\n",
      "Epoch 10/150\n",
      "----------\n",
      "train Loss: 1279.8263 Acc: 0.5704\n",
      "val Loss: 1147.5940 Acc: 0.4167\n",
      "\n",
      "Epoch 11/150\n",
      "----------\n",
      "train Loss: 1280.4145 Acc: 0.6479\n",
      "val Loss: 1110.0764 Acc: 0.6111\n",
      "\n",
      "Epoch 12/150\n",
      "----------\n",
      "train Loss: 1249.1877 Acc: 0.7535\n",
      "val Loss: 1001.1001 Acc: 0.8056\n",
      "\n",
      "Epoch 13/150\n",
      "----------\n",
      "train Loss: 1233.6251 Acc: 0.7887\n",
      "val Loss: 1004.7301 Acc: 0.7500\n",
      "\n",
      "Epoch 14/150\n",
      "----------\n",
      "train Loss: 1153.8329 Acc: 0.8592\n",
      "val Loss: 947.5052 Acc: 0.7500\n",
      "\n",
      "Epoch 15/150\n",
      "----------\n",
      "train Loss: 1211.0325 Acc: 0.7676\n",
      "val Loss: 1004.0121 Acc: 0.7222\n",
      "\n",
      "Epoch 16/150\n",
      "----------\n",
      "train Loss: 1110.2190 Acc: 0.8592\n",
      "val Loss: 947.4844 Acc: 0.8056\n",
      "\n",
      "Epoch 17/150\n",
      "----------\n",
      "train Loss: 1086.4256 Acc: 0.8803\n",
      "val Loss: 938.8996 Acc: 0.8333\n",
      "\n",
      "Epoch 18/150\n",
      "----------\n",
      "train Loss: 1064.2669 Acc: 0.9155\n",
      "val Loss: 933.3299 Acc: 0.8333\n",
      "\n",
      "Epoch 19/150\n",
      "----------\n",
      "train Loss: 1048.7428 Acc: 0.9014\n",
      "val Loss: 955.1430 Acc: 0.7778\n",
      "\n",
      "Epoch 20/150\n",
      "----------\n",
      "train Loss: 1039.6321 Acc: 0.9014\n",
      "val Loss: 953.0076 Acc: 0.8056\n",
      "\n",
      "Epoch 21/150\n",
      "----------\n",
      "train Loss: 1035.0088 Acc: 0.9155\n",
      "val Loss: 967.1727 Acc: 0.7778\n",
      "\n",
      "Epoch 22/150\n",
      "----------\n",
      "train Loss: 1030.2310 Acc: 0.9085\n",
      "val Loss: 983.2275 Acc: 0.7778\n",
      "\n",
      "Epoch 23/150\n",
      "----------\n",
      "train Loss: 1027.9334 Acc: 0.9366\n",
      "val Loss: 974.2366 Acc: 0.7778\n",
      "\n",
      "Epoch 24/150\n",
      "----------\n",
      "train Loss: 1025.6329 Acc: 0.9296\n",
      "val Loss: 968.3130 Acc: 0.7778\n",
      "\n",
      "Epoch 25/150\n",
      "----------\n",
      "train Loss: 1023.9165 Acc: 0.9296\n",
      "val Loss: 964.6585 Acc: 0.7778\n",
      "\n",
      "Epoch 26/150\n",
      "----------\n",
      "train Loss: 1022.8882 Acc: 0.9296\n",
      "val Loss: 961.7797 Acc: 0.7778\n",
      "\n",
      "Epoch 27/150\n",
      "----------\n",
      "train Loss: 1021.8976 Acc: 0.9296\n",
      "val Loss: 960.2084 Acc: 0.7778\n",
      "\n",
      "Epoch 28/150\n",
      "----------\n",
      "train Loss: 1021.4079 Acc: 0.9155\n",
      "val Loss: 960.4748 Acc: 0.7778\n",
      "\n",
      "Epoch 29/150\n",
      "----------\n",
      "train Loss: 1020.9750 Acc: 0.9155\n",
      "val Loss: 960.0611 Acc: 0.7778\n",
      "\n",
      "Epoch 30/150\n",
      "----------\n",
      "train Loss: 1020.1391 Acc: 0.9225\n",
      "val Loss: 960.0323 Acc: 0.7778\n",
      "\n",
      "Epoch 31/150\n",
      "----------\n",
      "train Loss: 1020.0871 Acc: 0.9225\n",
      "val Loss: 960.0079 Acc: 0.7778\n",
      "\n",
      "Epoch 32/150\n",
      "----------\n",
      "train Loss: 1020.0523 Acc: 0.9225\n",
      "val Loss: 959.9887 Acc: 0.7778\n",
      "\n",
      "Epoch 33/150\n",
      "----------\n",
      "train Loss: 1020.0097 Acc: 0.9225\n",
      "val Loss: 959.9696 Acc: 0.7778\n",
      "\n",
      "Epoch 34/150\n",
      "----------\n",
      "train Loss: 1019.9519 Acc: 0.9225\n",
      "val Loss: 959.9489 Acc: 0.7778\n",
      "\n",
      "Epoch 35/150\n",
      "----------\n",
      "train Loss: 1019.9153 Acc: 0.9225\n",
      "val Loss: 959.9270 Acc: 0.7778\n",
      "\n",
      "Epoch 36/150\n",
      "----------\n",
      "train Loss: 1019.8854 Acc: 0.9225\n",
      "val Loss: 959.9121 Acc: 0.7778\n",
      "\n",
      "Epoch 37/150\n",
      "----------\n",
      "train Loss: 1019.7846 Acc: 0.9225\n",
      "val Loss: 959.9103 Acc: 0.7778\n",
      "\n",
      "Epoch 38/150\n",
      "----------\n",
      "train Loss: 1019.7804 Acc: 0.9225\n",
      "val Loss: 959.9085 Acc: 0.7778\n",
      "\n",
      "Epoch 39/150\n",
      "----------\n",
      "train Loss: 1019.7764 Acc: 0.9225\n",
      "val Loss: 959.9067 Acc: 0.7778\n",
      "\n",
      "Epoch 40/150\n",
      "----------\n",
      "train Loss: 1019.7720 Acc: 0.9225\n",
      "val Loss: 959.9050 Acc: 0.7778\n",
      "\n",
      "Epoch 41/150\n",
      "----------\n",
      "train Loss: 1019.7682 Acc: 0.9225\n",
      "val Loss: 959.9032 Acc: 0.7778\n",
      "\n",
      "Epoch 42/150\n",
      "----------\n",
      "train Loss: 1019.7624 Acc: 0.9225\n",
      "val Loss: 959.9015 Acc: 0.7778\n",
      "\n",
      "Epoch 43/150\n",
      "----------\n",
      "train Loss: 1019.7586 Acc: 0.9225\n",
      "val Loss: 959.8997 Acc: 0.7778\n",
      "\n",
      "Epoch 44/150\n",
      "----------\n",
      "train Loss: 1019.7503 Acc: 0.9225\n",
      "val Loss: 959.8996 Acc: 0.7778\n",
      "\n",
      "Epoch 45/150\n",
      "----------\n",
      "train Loss: 1019.7499 Acc: 0.9225\n",
      "val Loss: 959.8994 Acc: 0.7778\n",
      "\n",
      "Epoch 46/150\n",
      "----------\n",
      "train Loss: 1019.7494 Acc: 0.9225\n",
      "val Loss: 959.8994 Acc: 0.7778\n",
      "\n",
      "Epoch 47/150\n",
      "----------\n",
      "train Loss: 1019.7490 Acc: 0.9225\n",
      "val Loss: 959.8992 Acc: 0.7778\n",
      "\n",
      "Epoch 48/150\n",
      "----------\n",
      "train Loss: 1019.7487 Acc: 0.9225\n",
      "val Loss: 959.8991 Acc: 0.7778\n",
      "\n",
      "Epoch 49/150\n",
      "----------\n",
      "train Loss: 1019.7482 Acc: 0.9225\n",
      "val Loss: 959.8990 Acc: 0.7778\n",
      "\n",
      "Epoch 50/150\n",
      "----------\n",
      "train Loss: 1019.7478 Acc: 0.9225\n",
      "val Loss: 959.8989 Acc: 0.7778\n",
      "\n",
      "Epoch 51/150\n",
      "----------\n",
      "train Loss: 1019.7470 Acc: 0.9225\n",
      "val Loss: 959.8990 Acc: 0.7778\n",
      "\n",
      "Epoch 52/150\n",
      "----------\n",
      "train Loss: 1019.7470 Acc: 0.9225\n",
      "val Loss: 959.8991 Acc: 0.7778\n",
      "\n",
      "Epoch 53/150\n",
      "----------\n",
      "train Loss: 1019.7470 Acc: 0.9225\n",
      "val Loss: 959.8992 Acc: 0.7778\n",
      "\n",
      "Epoch 54/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8993 Acc: 0.7778\n",
      "\n",
      "Epoch 55/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8994 Acc: 0.7778\n",
      "\n",
      "Epoch 56/150\n",
      "----------\n",
      "train Loss: 1019.7470 Acc: 0.9225\n",
      "val Loss: 959.8995 Acc: 0.7778\n",
      "\n",
      "Epoch 57/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8996 Acc: 0.7778\n",
      "\n",
      "Epoch 58/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8996 Acc: 0.7778\n",
      "\n",
      "Epoch 59/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8997 Acc: 0.7778\n",
      "\n",
      "Epoch 60/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8997 Acc: 0.7778\n",
      "\n",
      "Epoch 61/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8997 Acc: 0.7778\n",
      "\n",
      "Epoch 62/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 63/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 64/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 65/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 66/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 67/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 68/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 69/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 70/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 71/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 72/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 73/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 74/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 75/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 76/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 77/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 78/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 79/150\n",
      "----------\n",
      "train Loss: 1019.7469 Acc: 0.9225\n",
      "val Loss: 959.8998 Acc: 0.7778\n",
      "\n",
      "Epoch 80/150\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-b493057a1fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-181-7d88c996d605>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "binary = True\n",
    "\n",
    "full_dataset = ad_dataset(binary=binary)\n",
    "\n",
    "train_ratio = 0.8\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model = ad_model\n",
    "\n",
    "if not binary:\n",
    "    criterion = CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = BCELoss()\n",
    "    \n",
    "# optimizer = Adam(model.parameters())\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "dataloaders = dict(train=train_dataloader, val=val_dataloader)\n",
    "dataset_sizes = dict(train=len(train_dataloader), val=len(val_dataloader))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "best_model = train_model(model, criterion, optimizer, scheduler, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = osteoporosis_tf_dataset(data_file='../data/mlp_data/scaled_test_data.pickle', label_file=None, phase='test')\n",
    "\n",
    "for sample in test_dataset:\n",
    "#     print(sample['data'].size())\n",
    "    \n",
    "    input_ = sample['data']\n",
    "    outputs = best_model(input_)\n",
    "#     _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    print(int(outputs.data > 0.5) == 1 )\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
